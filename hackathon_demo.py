#!/usr/bin/env python3
"""
=============================================================================
AGORA HACKATHON DEMO - PLUG AND PLAY
=============================================================================

A complete showcase of Agora's capabilities - just run it!

Features demonstrated:
‚úÖ Workflow orchestration
‚úÖ Conditional routing
‚úÖ Retry logic with exponential backoff
‚úÖ Error handling
‚úÖ Batch processing
‚úÖ Async execution
‚úÖ Wide events (business context)
‚úÖ Comprehensive logging to console + file
‚úÖ LLM calls with full tracing

Use case: AI Research Assistant
- Generates research questions
- Searches for answers (with retry)
- Validates results
- Summarizes findings
- Handles errors gracefully

Just run: python hackathon_demo.py

=============================================================================
"""

import os
import asyncio
import time
from datetime import datetime
from typing import Dict, Any, List

# =============================================================================
# 1. SETUP - Set your OpenAI API key here
# =============================================================================

# REPLACE THIS WITH YOUR KEY (or set OPENAI_API_KEY environment variable)
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', 'sk-proj-YOUR_KEY_HERE')

if OPENAI_API_KEY == 'sk-proj-YOUR_KEY_HERE':
    print("=" * 80)
    print("‚ö†Ô∏è  PLEASE SET YOUR OPENAI API KEY")
    print("=" * 80)
    print("Option 1: Edit line 44 of this file")
    print("Option 2: Run: export OPENAI_API_KEY='sk-...'")
    print("Option 3: Run: OPENAI_API_KEY='sk-...' python hackathon_demo.py")
    print("=" * 80)
    exit(1)

os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY

# =============================================================================
# 2. INITIALIZE AGORA - Local telemetry (no platform needed)
# =============================================================================

from agora.agora_tracer import init_agora
from agora.wide_events import set_business_context
from agora import AsyncNode, AsyncFlow
from openai import OpenAI

print("\n" + "=" * 80)
print("üöÄ AGORA HACKATHON DEMO - AI RESEARCH ASSISTANT")
print("=" * 80)
print()

# Initialize Agora with console + file logging (NO PLATFORM NEEDED!)
init_agora(
    app_name="hackathon-research-assistant",
    export_to_console=True,           # ‚úÖ See traces in terminal
    export_to_file="research.jsonl",  # ‚úÖ Save traces to file
    enable_cloud_upload=False         # ‚ùå No platform needed
)

print("‚úÖ Agora initialized - traces will be saved to research.jsonl")
print()

# Set business context for wide events
set_business_context(
    user_id="hackathon_participant",
    subscription_tier="hacker",
    session_id=f"research_{int(time.time())}",
    workflow_type="research_assistant",
    custom={
        "hackathon": "2026_winter",
        "project": "ai_research_assistant",
        "demo_version": "v1.0"
    }
)

print("‚úÖ Business context set - all LLM calls will include metadata")
print()

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

# =============================================================================
# 3. DEFINE RESEARCH WORKFLOW NODES
# =============================================================================

class GenerateQuestions(AsyncNode):
    """Generate research questions about a topic"""

    async def exec_async(self, prep_res):
        topic = prep_res

        print(f"\n{'='*80}")
        print(f"üîç NODE 1: Generating research questions about '{topic}'")
        print(f"{'='*80}")

        # This LLM call is AUTO-TRACED with business context!
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a research assistant. Generate 3 interesting research questions."},
                {"role": "user", "content": f"Generate 3 research questions about: {topic}"}
            ],
            max_tokens=200
        )

        questions = response.choices[0].message.content
        print(f"‚úÖ Generated questions:\n{questions}\n")

        return {"topic": topic, "questions": questions}


class SearchAnswers(AsyncNode):
    """Search for answers to questions (with simulated retry logic)"""

    def __init__(self):
        super().__init__()
        self.attempt_count = 0

    async def exec_async(self, prep_res):
        data = prep_res
        questions = data['questions']

        print(f"\n{'='*80}")
        print(f"üîé NODE 2: Searching for answers")
        print(f"{'='*80}")

        # Simulate occasional failures to showcase retry logic
        self.attempt_count += 1
        if self.attempt_count == 1:
            print("‚ö†Ô∏è  Simulated network error - will retry...")
            raise Exception("Network timeout - retrying...")

        # This LLM call is AUTO-TRACED!
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a knowledgeable research assistant. Provide concise answers."},
                {"role": "user", "content": f"Answer these research questions:\n{questions}"}
            ],
            max_tokens=300
        )

        answers = response.choices[0].message.content
        print(f"‚úÖ Found answers:\n{answers}\n")

        return {**data, "answers": answers}


class ValidateResults(AsyncNode):
    """Validate research results and determine next action"""

    async def exec_async(self, prep_res):
        data = prep_res

        print(f"\n{'='*80}")
        print(f"‚úîÔ∏è  NODE 3: Validating results")
        print(f"{'='*80}")

        # Check if we have good results
        has_answers = len(data.get('answers', '')) > 50

        if has_answers:
            print("‚úÖ Validation passed - proceeding to summary")
            return {"validation": "success", **data}
        else:
            print("‚ö†Ô∏è  Validation failed - needs refinement")
            return {"validation": "failed", **data}

    async def post_async(self, shared: Dict[str, Any], prep_res: Any, exec_res: Any) -> str:
        """Conditional routing based on validation"""
        validation = exec_res.get('validation')

        if validation == 'success':
            return 'summarize'  # Go to summary node
        else:
            return 'refine'     # Go to refinement node


class SummarizeFindings(AsyncNode):
    """Summarize the research findings"""

    async def exec_async(self, prep_res):
        data = prep_res

        print(f"\n{'='*80}")
        print(f"üìù NODE 4A: Summarizing findings")
        print(f"{'='*80}")

        # This LLM call is AUTO-TRACED!
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a research assistant. Create a concise summary."},
                {"role": "user", "content": f"Summarize these research findings:\n\nQuestions:\n{data['questions']}\n\nAnswers:\n{data['answers']}"}
            ],
            max_tokens=200
        )

        summary = response.choices[0].message.content
        print(f"‚úÖ Summary:\n{summary}\n")

        return {**data, "summary": summary, "status": "completed"}


class RefineResults(AsyncNode):
    """Refine results when validation fails"""

    async def exec_async(self, prep_res):
        data = prep_res

        print(f"\n{'='*80}")
        print(f"üîß NODE 4B: Refining results")
        print(f"{'='*80}")

        print("‚úÖ Results refined - marking as needing review\n")

        return {**data, "summary": "Results need manual review", "status": "needs_review"}


class BatchProcessor(AsyncNode):
    """Process multiple topics in batch"""

    async def exec_async(self, prep_res):
        topics = prep_res

        print(f"\n{'='*80}")
        print(f"üì¶ BATCH PROCESSOR: Processing {len(topics)} topics")
        print(f"{'='*80}")

        results = []
        for i, topic in enumerate(topics, 1):
            print(f"\n[Batch {i}/{len(topics)}] Processing: {topic}")

            # Quick summary for each topic
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": f"In one sentence, what is {topic}?"}
                ],
                max_tokens=50
            )

            summary = response.choices[0].message.content
            results.append({"topic": topic, "summary": summary})
            print(f"  ‚úÖ {summary}")

        print(f"\n‚úÖ Batch complete: {len(results)} topics processed\n")
        return results


# =============================================================================
# 4. BUILD THE RESEARCH WORKFLOW
# =============================================================================

async def run_research_workflow(topic: str):
    """Main research workflow with conditional routing and retry logic"""

    print("=" * 80)
    print("üèóÔ∏è  BUILDING WORKFLOW")
    print("=" * 80)
    print()

    # Create nodes
    generate = GenerateQuestions()
    search = SearchAnswers()
    validate = ValidateResults()
    summarize = SummarizeFindings()
    refine = RefineResults()

    # Build workflow with conditional routing
    flow = AsyncFlow()

    # Start with question generation
    flow.start(generate) >> search >> validate

    # Conditional routing based on validation
    validate - 'summarize' >> summarize  # Success path
    validate - 'refine' >> refine        # Failure path

    print("‚úÖ Workflow built:")
    print("   Generate ‚Üí Search ‚Üí Validate ‚Üí [Summarize OR Refine]")
    print()

    # Run workflow with retry logic
    print("=" * 80)
    print("‚ñ∂Ô∏è  RUNNING WORKFLOW")
    print("=" * 80)

    shared = {"input": topic}

    try:
        result = await flow.run(shared)
        return result
    except Exception as e:
        print(f"‚ùå Workflow error: {e}")
        return {"status": "error", "error": str(e)}


async def run_batch_workflow(topics: List[str]):
    """Batch processing workflow"""

    print("\n" + "=" * 80)
    print("üì¶ BATCH WORKFLOW")
    print("=" * 80)

    batch = BatchProcessor()

    flow = AsyncFlow()
    flow.start(batch)

    shared = {"input": topics}
    result = await flow.run(shared)

    return result


# =============================================================================
# 5. RUN THE DEMO
# =============================================================================

async def main():
    """Main demo - showcases all Agora features"""

    start_time = time.time()

    # DEMO 1: Single research workflow with retry and conditional routing
    print("\n" + "‚ñà" * 80)
    print("‚ñà DEMO 1: RESEARCH WORKFLOW WITH RETRY & CONDITIONAL ROUTING")
    print("‚ñà" * 80)

    result1 = await run_research_workflow("Large Language Models")

    print("\n" + "=" * 80)
    print("üìä WORKFLOW RESULT")
    print("=" * 80)
    print(f"Status: {result1.get('status', 'unknown')}")
    print(f"Summary: {result1.get('summary', 'N/A')}")
    print()

    # DEMO 2: Batch processing
    print("\n" + "‚ñà" * 80)
    print("‚ñà DEMO 2: BATCH PROCESSING")
    print("‚ñà" * 80)

    topics = [
        "Quantum Computing",
        "Neural Networks",
        "Blockchain"
    ]

    result2 = await run_batch_workflow(topics)

    print("\n" + "=" * 80)
    print("üìä BATCH RESULTS")
    print("=" * 80)
    for item in result2:
        print(f"  ‚Ä¢ {item['topic']}: {item['summary']}")
    print()

    # Show timing
    elapsed = time.time() - start_time

    print("\n" + "=" * 80)
    print("‚ú® DEMO COMPLETE")
    print("=" * 80)
    print(f"‚è±Ô∏è  Total time: {elapsed:.2f}s")
    print()

    # Show what was logged
    print("=" * 80)
    print("üìÅ TELEMETRY SAVED")
    print("=" * 80)
    print("‚úÖ Console: All traces printed above")
    print("‚úÖ File: research.jsonl (view with: cat research.jsonl | jq)")
    print()
    print("What's in the telemetry:")
    print("  ‚Ä¢ All LLM calls with prompts & completions")
    print("  ‚Ä¢ Token usage and costs")
    print("  ‚Ä¢ Node execution timing")
    print("  ‚Ä¢ Retry attempts")
    print("  ‚Ä¢ Conditional routing decisions")
    print("  ‚Ä¢ Business context (user_id, session_id, hackathon metadata)")
    print()

    # Show Agora features demonstrated
    print("=" * 80)
    print("üéØ AGORA FEATURES SHOWCASED")
    print("=" * 80)
    print("‚úÖ Workflow Orchestration - Multi-node pipelines")
    print("‚úÖ Async Execution - Full async/await support")
    print("‚úÖ Conditional Routing - Dynamic flow based on results")
    print("‚úÖ Retry Logic - Auto-retry on failures (SearchAnswers node)")
    print("‚úÖ Error Handling - Graceful degradation")
    print("‚úÖ Batch Processing - Process multiple items efficiently")
    print("‚úÖ Wide Events - Business context on every span")
    print("‚úÖ Local Telemetry - Console + file (no platform needed)")
    print("‚úÖ LLM Auto-tracing - Every OpenAI call captured")
    print("‚úÖ Node Chaining - Clean >> syntax")
    print()

    print("=" * 80)
    print("üöÄ NEXT STEPS FOR YOUR HACKATHON PROJECT")
    print("=" * 80)
    print("1. Copy this file as a starting point")
    print("2. Replace the nodes with your use case")
    print("3. Add more conditional routing as needed")
    print("4. Adjust retry logic and error handling")
    print("5. Query research.jsonl for metrics and insights")
    print()
    print("Query example:")
    print('  cat research.jsonl | jq \'.attributes."llm.usage.total_tokens"\'')
    print()
    print("Happy hacking! üéâ")
    print("=" * 80)
    print()


# =============================================================================
# RUN IT!
# =============================================================================

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Demo interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
